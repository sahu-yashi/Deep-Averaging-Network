{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIRCIFfEyMyI",
        "outputId": "eef24e8e-ba71-44fe-ee81-a38ed8e6a284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "H42-cV4Npi7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords"
      ],
      "metadata": {
        "id": "gFVKgKnh6dgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TMV7luNQ0J6",
        "outputId": "db555a9b-a02c-4732-e19a-e97d9145ab1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rheEZof9pWAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "from torch.utils.data import TensorDataset,DataLoader"
      ],
      "metadata": {
        "id": "yVSLVH4_Urf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "3do4Ghta3rUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a0aa9d-c984-434d-e0de-16cb57cd56c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 15.8 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 60.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions"
      ],
      "metadata": {
        "id": "uFNNsCur4YQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "TlkmKP5uPzre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLKBjs1m4343",
        "outputId": "a4093460-cd9c-4a42-a383-4c4b24f74e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "import fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOLfxtk48NlZ",
        "outputId": "3f41ba4c-0466-4a5b-af94-0b25bf22dbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3164515 sha256=a39a43720d4c25b56242007371d7bb655547816887dc709d854fd81ae37ed483\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation\n",
        "import re\n",
        "import fasttext.util"
      ],
      "metadata": {
        "id": "bTK7GPtCPbIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_neg = pd.read_csv(r\"/content/drive/MyDrive/DAN/neg.txt\",sep='\\t',header=None)\n",
        "df_pos = pd.read_csv(r\"/content/drive/MyDrive/DAN/pos.txt\",sep='\\t',header=None)\n",
        "df_neg.columns=['review']\n",
        "df_pos.columns=['review']\n",
        "lst_0 = [0] * len(df_neg)\n",
        "lst_1=[1]*len(df_pos)\n",
        "df_neg['sentiment'] = lst_0\n",
        "df_pos['sentiment']=lst_1\n",
        "df=pd.concat([df_neg,df_pos], ignore_index=True)"
      ],
      "metadata": {
        "id": "eALFCj_j3kGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b8ff42-422d-41fd-99d8-320fa00dde44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e41a2b7a4dcb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df_neg = pd.read_csv(r\"/content/drive/MyDrive/DAN/neg.txt\",sep='\\t',header=None.skiprows=0)\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Uty2hZMac5sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cleaning:\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "  def remove_punctuation(self,text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "  def remove_html(self,data):\n",
        "    html_tag=re.compile(r'<.*?>')\n",
        "    data=html_tag.sub(r'',data)\n",
        "    return data\n",
        "  def remove_url(self,data):\n",
        "    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n",
        "    data=url_clean.sub(r'',data)\n",
        "    return data\n",
        "  def remove_stopwords(self,text):\n",
        "    \n",
        "    filtered_sentence = remove_stopwords(text)\n",
        "    return filtered_sentence\n",
        "        \n",
        "\n",
        "  def clean_data(self):\n",
        "    self.df['review']= self.df['review'].apply(lambda x:self.remove_punctuation(x))\n",
        "    self.df['review']=self.df['review'].apply(lambda z: self.remove_html(z))\n",
        "    self.df['review']=self.df['review'].apply(lambda z: self.remove_url(z))\n",
        "    self.df['review']=self.df['review'].apply(lambda z: contractions.fix(z) )\n",
        "    # self.df['review']= df['review'].apply(lambda x:self.remove_stopwords(x))\n",
        "   \n",
        "    self.df['review']= self.df['review'].apply(lambda x: x.lower())\n",
        "    self.df['review'] = self.df.apply(lambda row: nltk.word_tokenize(row['review']), axis=1)\n",
        "    # self.df['review']= df['review'].apply(lambda x:self.remove_stopwords(x))\n",
        "    return self.df\n",
        "\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "6IMIOkHxZucO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj=cleaning(df)\n",
        "df=obj.clean_data()\n",
        "\n"
      ],
      "metadata": {
        "id": "qyy4xP3Weve2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5E6F_tlScio",
        "outputId": "5ba15286-69ad-4933-b3a0-a10714bad2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=[]\n",
        "for i in range(len(df['review'])):\n",
        "  \n",
        "  list=[]\n",
        "  for j in range(len(df['review'][i])):\n",
        "    word=df['review'][i][j]\n",
        "   \n",
        "    vector=ft.get_word_vector(word)\n",
        "    list.append(vector)\n",
        "  embeddings.append(np.mean(list,axis=0))\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "lM_8pwjSTO4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df=pd.DataFrame(embeddings)\n",
        "embeddings_df = embeddings_df.join(df['sentiment'])\n"
      ],
      "metadata": {
        "id": "R4Z1fMJFwQp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(embeddings_df, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "K_iLMXWKn4iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x= train.iloc[:,:300].to_numpy()\n",
        "train_y=train.iloc[:,-1].to_numpy()\n",
        "val_x= val.iloc[:,:300].to_numpy()\n",
        "val_y=val.iloc[:,-1].to_numpy()"
      ],
      "metadata": {
        "id": "dMEo8Y10zaVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x =torch.from_numpy(train_x)\n",
        "train_y =torch.from_numpy(train_y)\n",
        "val_x =torch.from_numpy(val_x)\n",
        "val_y =torch.from_numpy(val_y)\n"
      ],
      "metadata": {
        "id": "X3vSUz331XQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=TensorDataset(train_x,train_y)"
      ],
      "metadata": {
        "id": "p9DUIfDZ3Wb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset=TensorDataset(val_x,val_y)"
      ],
      "metadata": {
        "id": "8VDEUg3d3r5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 300\n",
        "hidden_size = [128, 64]\n",
        "num_classes = 2\n",
        "num_epochs = 100\n",
        "batch_size = 100\n",
        "learning_rate = 0.01 "
      ],
      "metadata": {
        "id": "ad3YuJO-2jiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                            batch_size=batch_size, \n",
        "                                            shuffle=True)"
      ],
      "metadata": {
        "id": "uJR4wYcE21-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                            batch_size=batch_size, \n",
        "                                            shuffle=True)"
      ],
      "metadata": {
        "id": "Qur71QPg9hf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "     def __init__(self, input_size, hidden_size, num_classes):\n",
        "         super(NeuralNet, self).__init__()\n",
        "         self.input_size = input_size\n",
        "         self.l1 = nn.Linear(input_size, hidden_size[0]) \n",
        "         self.relu1 = nn.Tanh()\n",
        "         self.l2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
        "         self.relu2=nn.Tanh()\n",
        "         self.l3 = nn.Linear(hidden_size[1],num_classes)\n",
        "         self.softmax=nn.Softmax(dim=1)\n",
        "     def forward(self, x):\n",
        "         out = self.l1(x)\n",
        "         out = self.relu1(out)\n",
        "         out = self.l2(out)\n",
        "         out = self.relu2(out)\n",
        "         out = self.l3(out)\n",
        "         out = self.softmax(out)\n",
        "       \n",
        "         return out\n",
        "        \n"
      ],
      "metadata": {
        "id": "f7tlyxSnTNvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "clr_D1qx6PQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "     train_loss=0\n",
        "     val_loss=0\n",
        "     acc=0\n",
        "     for i, (review, sentiment) in enumerate(train_loader):\n",
        "        outputs = model(review)\n",
        "        loss = criterion(outputs, sentiment)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss=train_loss+loss.item()\n",
        "        # print (f'Epoch [{epoch+1}/{num_epochs}], Step[{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "     for batch in val_loader:\n",
        "          x, y = batch\n",
        "          outputs = model(x)\n",
        "          loss = criterion(outputs, sentiment)\n",
        "          # optimizer.zero_grad()\n",
        "          val_loss=val_loss+loss.item()\n",
        "          out = torch.argmax(outputs, dim=1)\n",
        "         \n",
        "          acc =acc+ torch.sum(out == y)\n",
        "\n",
        "     \n",
        "     print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {val_loss / len(val_loader)}')\n",
        "     print(\"accuracy on validation dataset: \",(acc/len(val_dataset)))"
      ],
      "metadata": {
        "id": "nBVYOkxE643x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}